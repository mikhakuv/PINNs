[Теоретическая работа на эту тему от создателей SPINN](https://proceedings.neurips.cc/paper_files/paper/2024/file/00532321a253959cedc4f971b5524131-Paper-Conference.pdf):  
"Основная теорема показывает, что увеличение порядка или размерности уравнения экспоненциально увеличивает требование к числу нейронов в каждом слое нейросети, что теоретически указывает на то, что PINN сложнее оптимизировать для уравнений высокого порядка или высокой размерности. Мы также обосновываем, что порядок PDE усиливает негативное влияние размерности." Но в этой статье рассматривается только случай, когда функция активации ReLu.

Применения PINN для решения уравнений с производными высокого порядка:  

* [FO-PINN](https://pdf.sciencedirectassets.com/271096/1-s2.0-S0955799725X00047/1-s2.0-S0955799725000499/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHcaCXVzLWVhc3QtMSJHMEUCIAOPNViqvsOj0zH9QLYRcfRXbIw2VrxDI2PSwqtxpTwXAiEA0nCcTExZIw3HUW0G8tiEClykt%2BpXeS8j5KaxuPGDN%2FEqswUIEBAFGgwwNTkwMDM1NDY4NjUiDHqRsGOQ6cpczCStDCqQBQh14D64wyQH4Bl6xd8%2BpAC2M9K3wbe0jyQigsO8BU01vpIcrOeiOvoXTczJOVQwwFR2iCan3etQtWMhQdUvui0KRrtiwYYTsms%2BJuyz2SSbiMWyD%2FNTeHImyw53yivwhFptMjBiK%2FdkC8UyaOnLpSFzaHFTQ7w8n4t7GQzYD%2FNAT0JoCumvsfDyg8JStMWfLTFDHTnUyjoigeAmaFh%2BO%2F0dO7nlmu2VbWJ%2BAD4uOfGyBZ4PKnO%2F4w0IhKsed2ZCnkXMmt16eENXmSWtBh15JVUYMRylvwbk4A9bSlgYNSLOl4xlXN93bM4NGqJgy4BEXuSPHkMmKdlp4Kz1WmQsco4O5lDywUnZY5%2BpooI6vY4pTrpjvf3mmLvzAbbMbOnfrIco8%2FDAVC8UUCBlP1rlL9nijgpGZiPcq71vXIUrlXDiE06jzJX4h9SQ%2FTm7EPj%2FEKbLBHsRE8ZG4QVm7AT5A5lWRN0%2FOIaKQfQyp75P%2FMWVgU3%2BROh3%2Fxc3kglJSJ2o9hVHHJn7dtVyCVBpRlJEp41DYN0JBtAEymrAtCvXDrgEg2nWi6YbI5bGefriBiDtx%2FF%2BpFfsyHSEKq94F%2FTQzte4OlamdClVEtZumzK6F0GUTpk3ymIEXK3kvIcoYJjWOvo%2Fqf%2Fhqs4hoyAxwrZ9O4PC3vT6NnmwaYbWrg2M4WXkPjLcH%2BAv%2Fb6FJ%2B2ehRtbMeT07t9Vp%2FAeY3EDs3UklgEU43fMXf5fQvYVit5mEPHYXuTOTOQtSZDzRjw2k5gcGGibGvjhkuZIO8EvZP6qtDqXS4gTKOaRK2JtdShxPsK6CtUDdhtYfCAsvSYbdKjSZLeVuVZ8rt2mfqaeh8f0f53v%2FIOX%2BS%2F9bP4u6IeVXqa1MM64p8AGOrEBP%2F0g2%2BfiRse4XUSPwNo7C3zkw72Wke6B3uCyX6QLc9rmNlmxeVc3UxGmcDOhnLynIr6PdxNCvkcTFKhhU44lflPj2XH4g7FM%2BMM%2BrdgQTR9OQYsuVSNNedcKFYf3JYdj49WsYSxpzbosO1DoEyM7bIjFifAKs12NH%2FVHEuM0dRaWWZ6dEzq2OZkXRvkAz6anz8gc3ZRMO1R3tXF8EWbb2RcwzHYb3G6aiCSncLQpEdum&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250424T071511Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYXL6C56YG%2F20250424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=74e8d1ad4567e95c970478840ade93b306dd62a1dcb00eb6197d510e736923fc&hash=aef4da82fadd079b97be017fe528d50729adb2bbb081bc72ca6f1d8ca2713ef2&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0955799725000499&tid=spdf-783564e7-6a93-4fa2-98ab-3f7a509e8104&sid=d6858e1943f07240397b478-37d1218bd64fgxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=081359575450040707065e&rr=9353ceda9efce498&cc=ru):
выход нейросети содержит не только $u$, но и другие выходы, которые аппроксимируют производные: $a$, $b$, итд. При этом в Loss добавляются слагаемые $\text{mean}((\frac{\partial u}{\partial x} - a)^2)$, $\text{mean}((\frac{\partial^2 u}{\partial x^2} - b)^2)$, итд. В итоге в формулировке уравнения остаются только производные первого порядка. В работе [A-PINN](https://pdf.sciencedirectassets.com/272570/1-s2.0-S0021999122X00120/1-s2.0-S0021999122003229/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHgaCXVzLWVhc3QtMSJHMEUCIFHUSpqCwy%2FBsxMFicGkGuZCPyH%2B8JlRU0mxQ6UW5Aa9AiEA40E%2FnGbLacPSbO92GdUsBSpemT8OW1%2FMPz6MporKvVgqsQUIEBAFGgwwNTkwMDM1NDY4NjUiDEPTHr1g4FD08fMfhyqOBdgCDwtYE1SAxzEIlsfaqZM%2Bhw2rirF6SvyWjkG7u7jYbepETfxcRCLmVxief%2BeoEhijYZ%2FeBB8LJPwja9MnqlYtyUJHepXPeXXZoEeOu1foTPl99WwO7Q68JkOArxHTI9MeWMkgVijWO%2Fb9lg6jQi6KhPn5NmbHWMVWQhXh7vj%2FvStlqAYHPSz8Rcd3RxvZlkuhP762NpsP202mlGeEOzlPpYzOCIF6YNrM13Vj95cRZkUQLTZp4VIZl5Gd8dhejCF4OQJB3w%2FVn0IA0b%2Bv8lm4uJy67cXJ88fyNN0VODe9KZzVf46ijgjH58J3BAyR%2FZ3Lgfb80vBM3E8PRZHXQyVByK7Q6WXpdPMFlIYN47ytqBoLFAwO9VYI%2Bk7GJPVpnk%2BQRE1PPyZ3Du0aTq3HMEqMjJYWhKqr9rtjbYcAFW8WihbVmzRmTS1q%2BOS23IBHAwZ403AihKRmrcRSC79BpJqI6Bgp6L%2B5x%2BW2Ly5isEjD%2FSMg4rcwwaI7cZZqfQeSu%2BZ%2FaUC9IuUBkqCMJKQGV%2FZcVcUgarY0v0Ccrj5m8b7knXQqft%2BtnnykrkCslSTxk2M8PjXD2g5JlxHlv06kRGxkMQ3S0VsRnFbWjlcqrABVxzBeBSiITGTMpcI%2BGF39s5UK03lWAb%2BxOVE3U3m9pzgz%2BBsQSxZU7aq3TN1dzi8D1K80GzIC7nqA8wM7F8NLWkt%2BnhKEe0VZmnyNxbvy%2Bh06rMo9vzm%2BjlECncx4RDk4GArjUEvP4UHcRSFJeuo%2FhP3WblFQ5iHfOTEQj1OdV%2BQN8q4DbhM1HAo9tORmBLrGbTObD%2FGWSWdI6TpYHyHBXsTIbBEQMCy8K5kuxAucR6i05gkmemQamF0HV77LfTDOyqfABjqxAYakry4CSWYFVIf8d13esRGScWbP4KiMnCyt036Vubqsh%2F3SlvNjSuXN4ZLeQ4mLJ9AtJIhipMbELm%2FjsN07CJce%2FbPWciy7aTkqZT0Xvc2X5NLuXIG53gMXktBqEzKk%2FO1yojOfLjuVZC0cmboImRKZkumQv1srf2afxKX9gaCN7sqGzOueyjITjDSHQNf0GzFbcjVgWHCq3LhQw%2Fo2eJTKXb9Qt8hUDqAbSPjto4zaAw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250424T074405Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3MS7U7ZP%2F20250424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=97866c832cffdd0e20ea53d78e882df2ea37ce16871fe79435132b9ab1e23c86&hash=72609829269b9af4fbbc437d24b34e9390c0f200ca53112343ce58db42a6c3af&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0021999122003229&tid=spdf-5264f000-a9ed-468a-9f08-9768e431c3c2&sid=d6858e1943f07240397b478-37d1218bd64fgxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=081359575450015b510255&rr=9353f92e2c28f116&cc=ru) есть похожая идея, но там она используется для решения интегро-дифференциальных уравнений.  
* [CAN-PINN](https://arxiv.org/pdf/2110.15832):  
вместо вычисления производных с помощью алгоритма численного дифференцирования, предлагается определять их приближённо, как в разностных схемах: $\frac{\partial f}{\partial x} = \frac{f(x+\Delta x) - f(x-\Delta x)}{2\Delta x}$. Также предлагается комбинировать вычисленные разным способом производные для увеличения точности: $\frac{\partial f}{\partial x} = f_x(x) + \frac{f(x) - f(x-\Delta x)}{\Delta x} - \frac{1}{2}(f_x(x)+f_x(x-\Delta x))$.
* [SHoP](https://ojs.aaai.org/index.php/AAAI/article/view/29535):
Авторы предлагают новый способ вычисления производных, представляя нейросеть как сложную функцию вида $f(g(x))$, для которой можно рекуррентным способом найти матрицу соотношений между векторами $(\frac{\partial f}{\partial x},\dots \frac{\partial^n f}{\partial x^n})^T$ и $(\frac{\partial f}{\partial g},\dots \frac{\partial^n f}{\partial g^n})^T$. В качестве примеров рассматриваются: одномерное ур-е гармонического осциллятора с производной 4 степени, двумерное бигармоническое ур-е с производной 4 степени, двумерное ур-е Гельмгольца с производной 8 порядка, трёхмерное ур-е теплопроводности с производной 4 степени (тем не менее, во всех случаях решения простые). Помимо этого, авторы предлагают идею перехода от обученной нейросети к ряду Тейлора, который они считают более понятным.

Применения PINN для решения уравнений с сильной нелинейностью:  

* [CV-PINN](https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.7.013164):
Для решения уравнений в комплексных числах авторы предлагают использовать нейросети с использованием комплексных весов, в то время как с нелинейностью и размером области предлагается бороться с помощью нового метода генерации точек (Predictive Dynamic Monitoring). Идея использования комплексной нейросети реализована следующим образом: входные переменные $x$ и $t$ насильно делаются комплексными, причём предлагается их комплексную часть сделать равной действительной. Далее они обрабатываются нейросетью, в которой нейроны являются комплексными: имеют комплексные веса и комплексные смещения. При этом на самом деле производятся только действительные вычисления (см картинку), что позволяет избежать проблем при автодифференцировании. Метод генерации точек коллокации следующий: посчитать изменение выхода нейросети на разных этапах обучения, в заданных прямоугольных областях вычислить средний модуль этого изменения, на основе посчитанных средних распределить плотности точек по областям (см картинку). Внутри каждой из областей генерация производится с помощью LHS. В результатах авторы указывают, что по сравнению с обычным PINN, ошибка снизилась на 32.6%. Также у статьи доступен [github](https://github.com/Lei-Zhang1227/CV-PINN).

<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/derivatives_literature_illustration_1.png" width="400" height="250">  

<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/derivatives_literature_illustration_2.png" width="700" height="400">  

* [LM algorithm](https://proceedings.neurips.cc/paper_files/paper/2024/file/49f07f600ca532a89ef4fa0618bb78c1-Paper-Conference.pdf):
С точки зрения NTK (Neural Tangent Kernel) авторы рассматривают чем отличается задача решения линейного уравнения от нелинейного и приходят к выводу, что предположения которые ранее работали для линейного случая, для нелинейного не выполняются. В связи с этим, предлагается изменить алгоритм оптимизации и сделать так, чтобы при оптимизации учитывался не только Якобиан (матрица первых производных), но и Гессиан (матрица вторых производных). Авторы предлагают использовать алгоритм LM (Levenberg-Marquardt), который по их словам имеет превосходство над квазиньютоновскими методами (к ним относится в том числе и LBFGS). Важно, что к недостаткам используемого метода авторы относят замедление алгоритма с ростом параметров нейросети, что мешает напрямую решать задачи на больших областях.

Прочие работы:  
* [Energy natural gradient descent](https://arxiv.org/pdf/2302.13163):


* [PINN for nonlinear dynamics in fiber optics](https://arxiv.org/pdf/2109.00526):
